\chapter{Introduction}\label{cha:intro}

\dropcap{F}{lying} insects perform remarkable feats of agile navigation with brains containing less than a million neurons and consuming mere microwatts of power. A honeybee can hover, avoid obstacles, and land on flowers while processing visual information at microsecond timescales---all with a brain smaller than a grain of rice~\cite{decroon2022insectinspired}. In contrast, modern autonomous flying robots typically require tens of watts of power and hundreds of grams of computational hardware to achieve even basic autonomous flight~\cite{nvidia}. This stark difference suggests that current approaches to robot vision may be fundamentally misaligned with the constraints faced by highly resource-limited platforms. The question then becomes: can we rethink robot vision from the ground up to better match the sparse, asynchronous, and energy-efficient principles exhibited by biological systems?

Over the past decade, deep artificial neural networks (ANNs) have revolutionized computer vision, achieving state-of-the-art performance on tasks ranging from image classification~\cite{he2016deep} to optical flow estimation~\cite{ilg2017flownet, sun2018pwcnet, teed2020raft} to monocular depth prediction~\cite{godard2017unsupervised, garg2016unsupervised}. However, this high accuracy typically relies on substantial neural network sizes that require quite heavy and power-hungry processing hardware. Even compact vision models demand tens of watts of power and rely on frame-based cameras operating at 30-60 Hz, producing dense image data that must be processed in its entirety~\cite{nvidia}. This limits the number of tasks that can be performed by larger ground robots, and even prevents deployment on smaller robots with highly stringent resource constraints, like small flying drones. For such platforms, where every gram of payload and every milliwatt of power consumption directly impact flight time and agility, a paradigm shift in how we approach robot vision becomes necessary.

One promising direction lies in rethinking not just the processing, but the sensing itself. Event cameras represent a fundamentally different approach to vision, one that aligns more closely with biological principles and resource constraints. Unlike traditional frame-based cameras that capture complete images at fixed intervals, event cameras operate asynchronously, with each pixel independently detecting brightness changes and outputting events at microsecond resolution~\cite{gallego2020eventbased}. This bio-inspired sensing paradigm offers several key advantages for flying robots: temporal resolution three orders of magnitude higher than standard cameras (microseconds vs. milliseconds), power consumption measured in milliwatts rather than watts, and sparse, asynchronous output that naturally compresses the visual information to only the changing parts of the scene. The high temporal resolution enables low-latency perception and decision-making on agile but resource-constrained platforms such as small drones~\cite{brandli2014240}. However, the asynchronous and sparse nature of event data also means that traditional computer vision pipelines, designed for dense frame-based images, do not transfer directly to this new sensing modality.

The sparse, asynchronous nature of event data suggests a natural match with neuromorphic processing architectures. Neuromorphic computing, inspired by the structure and function of biological neural networks, processes information through spiking neural networks (SNNs) that communicate via discrete spikes rather than continuous values. Like biological neurons, neuromorphic processors perform sparse, event-driven computation, promising orders of magnitude lower energy consumption compared to traditional von Neumann architectures~\cite{davies2021advancing, merolla2014million}. State-of-the-art neuromorphic processors such as Intel's Loihi chip~\cite{davies2018loihi} and IBM's TrueNorth~\cite{merolla2014million} demonstrate impressive efficiency, operating in the milliwatt range while supporting hundreds of thousands of neurons. When coupled with event cameras, neuromorphic processors could enable a fully asynchronous, event-driven vision pipeline---where individual brightness changes trigger spikes that propagate through the network without any frame-based accumulation or processing. Such a pipeline would match the sparse and asynchronous nature of both sensing and biological neural processing.

Despite this promise, significant challenges have prevented the widespread adoption of neuromorphic vision systems for robotics. Training spiking neural networks remains substantially more difficult than training traditional ANNs, primarily due to the non-differentiable nature of the spiking activation function. While surrogate gradient methods~\cite{neftci2019surrogate, zenke2021remarkable} have enabled backpropagation through SNNs, additional challenges arise from sparse, binary spike activity that can lead to gradient vanishing, as well as more complex neuronal dynamics that require careful initialization and optimization~\cite{pfeiffer2018deep, tavanaei2019deep}. Furthermore, current neuromorphic processors impose strict constraints on network size: even state-of-the-art chips like Loihi support only 262,144 neurons---orders of magnitude fewer than the millions typically required by deep neural networks for complex vision tasks. As a consequence, SNNs have not yet been successfully applied to the kind of complex, large-scale visual perception problems that are routine for frame-based deep learning. Prior work with SNNs has largely been limited to relatively simple tasks such as classification on small datasets like neuromorphic MNIST~\cite{orchard2015converting} or DVS128 Gesture~\cite{amir2017low}. Dense regression problems such as optical flow or depth estimation, which require per-pixel predictions and sophisticated temporal integration, have remained largely out of reach for directly-trained SNNs.

Flying robots, and particularly small drones, represent both the most challenging and the most promising application domain for neuromorphic event-based vision. These platforms face extreme constraints in size, weight, and power (SWaP), where every gram of payload and every milliwatt of power consumption directly affects flight time, maneuverability, and the ability to carry additional sensors or actuators~\cite{karasek2018tailless}. Recent years have seen an increasing focus on flying robots because they need to react quickly while being extremely restricted in terms of SWaP~\cite{decroon2016monocular}. Flight control requires high-frequency perception and fast control loops---typically exceeding 100 Hz---to maintain stability and respond to disturbances in dynamic environments. Current autonomous flight systems rely heavily on computationally intensive vision algorithms running on conventional processors, or alternatively depend on external positioning systems (such as motion capture) or inertial measurement units (IMUs) for state estimation. Yet flying insects achieve remarkable flight agility and robustness with minimal sensory and computational resources, suggesting that vision-based control with highly efficient processing is not only possible but may be the natural solution~\cite{decroon2022insectinspired}. The combination of event cameras' low latency and power consumption with neuromorphic processors' efficient spike-based computation holds particular promise for closing this gap between biological and robotic flight capabilities.

A critical challenge in applying neural networks to event-based vision is the availability of training data with ground truth labels. For event cameras operating at microsecond temporal resolution, obtaining ground truth for tasks like optical flow or depth estimation at comparable rates is impractical. Traditional motion capture systems or depth sensors typically operate at 10-20 Hz, creating a fundamental mismatch between the sensor capabilities and the supervision signal~\cite{zhu2018multivehicle}. This challenge has motivated the development of self-supervised learning (SSL) approaches that can learn directly from the event stream without external ground truth. The contrast maximization framework~\cite{gallego2018unifying, gallego2019focus} provides a particularly elegant solution: it leverages the principle that brightness edges should appear sharp when events are correctly motion-compensated, and blurred otherwise. This geometric constraint allows learning optical flow, depth, and ego-motion from events alone~\cite{zhu2019unsupervised}. Moreover, self-supervised learning offers an additional advantage particularly relevant for robotics: since no ground truth is needed, learning and prediction can run at higher frequencies, limited only by computational resources rather than sensor synchronization~\cite{paredes-valles2023taming}. Perhaps most importantly, SSL can in principle be performed in the operational environment of a robot or other edge device. Such online self-supervised learning greatly reduces the need for generalization of the learned model, as training happens directly on data sampled from the test distribution---the actual environment in which the robot operates.

Translating optical flow estimates into robot control commands presents its own set of challenges. For flying robots, optical flow provides rich information about ego-motion and the 3D structure of the environment, which can be used for tasks ranging from obstacle avoidance to velocity control to landing~\cite{decroon2016monocular}. However, the mapping from visual perception to motor commands is non-trivial and depends on the specific robot platform, actuation capabilities, and task requirements. While simple linear mappings or hand-tuned control laws can work for constrained scenarios, learned controllers offer the promise of automatically discovering effective perception-to-action mappings. Evolutionary algorithms provide one approach to this problem, allowing controllers to be optimized in simulation and then transferred to real robots~\cite{dupeyroux2021}. The sim-to-real transfer challenge---ensuring that controllers trained in simulation work on physical platforms---remains a central concern, particularly when the sensory inputs (such as event streams) may differ between simulation and reality. For neuromorphic systems, this challenge is compounded by the need to design controllers that respect hardware constraints, such as the limited number of neurons available on neuromorphic chips.

Moving from isolated algorithms to complete, deployed robotic systems requires addressing numerous integration challenges that are often overlooked in purely algorithmic research. Real-world deployment demands not just high accuracy on benchmark datasets, but robustness to varying conditions, real-time performance guarantees, and integration with the full perception-to-action pipeline. Hardware constraints---processing power, memory, communication bandwidth, power budgets---impose hard limits that require careful co-design of algorithms and systems. Software infrastructure must handle asynchronous event streams, manage timing constraints, and coordinate multiple processing stages. Testing and validation must move beyond simulation to physical robots operating in real environments, where unexpected corner cases and failure modes emerge. This gap between laboratory demonstrations on recorded datasets and fully autonomous systems operating in the real world remains one of the largest barriers to practical deployment of neuromorphic vision. Throughout this thesis, emphasis is placed on developing complete, functioning systems that run on physical flying robots, not just algorithms evaluated on datasets.

The convergence of event-based vision, neuromorphic computing, and self-supervised learning opens new possibilities for resource-efficient robot vision. However, significant gaps remain between the promise of these technologies and their practical deployment on autonomous robots. Spiking neural networks have not yet been demonstrated on dense, complex visual tasks like optical flow estimation. Neuromorphic processors remain too constrained for networks trained with traditional methods. Self-supervised learning frameworks are too computationally expensive for online, on-board training. And despite the theoretical possibility of vision-only control, robots continue to depend on inertial sensors. This dissertation addresses these gaps through a progression of contributions that culminate in fully autonomous, neuromorphic vision systems for flying robots. The work proceeds through four stages: first, establishing that SNNs can learn complex visual tasks through self-supervised learning; second, deploying a complete neuromorphic vision-to-control pipeline on a flying robot; third, enabling online self-supervised learning during flight; and fourth, demonstrating that vision alone can replace inertial sensors for flight control. Each stage builds upon the previous one, progressing from fundamental capabilities to practical deployment to online adaptation to ultimate sensor minimalism.

\section{Problem statement and research questions}

The challenges outlined above motivate the central research question of this dissertation:

\tcbset{colframe=black!90!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Main Research Question}}]
    How can robots learn to perceive the world with event-based vision in a way that enables effective acting in that world?
\end{tcolorbox}

This question emphasizes that solutions must not only achieve high perceptual accuracy but also be deployable with low enough latency and computational requirements to be useful for real-time robot control. To make this challenge concrete and tractable, we focus on the specific problem of autonomous flight with event-based vision:

\tcbset{colframe=black!90!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Problem Statement}}]
    How can a flying robot learn to navigate autonomously when it only has event-based vision?
\end{tcolorbox}

Flying robots with only event-based vision represent an extreme test case: they combine the strictest resource constraints with the highest demands for low-latency, robust perception. Addressing this problem requires tackling challenges across the full stack, from learning algorithms to hardware deployment to online adaptation. We decompose this overarching problem into four specific research questions, each of which corresponds to one of the main chapters of this thesis.

The first fundamental question is whether spiking neural networks can learn complex visual tasks at all:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 1}}]
    How can spiking neural networks learn to estimate dense optical flow from event streams in a self-supervised manner?
\end{tcolorbox}

Optical flow estimation serves as an ideal test case: it is a dense regression problem requiring per-pixel predictions and sophisticated temporal integration, representing the kind of complex task that has traditionally been out of reach for SNNs. Self-supervised learning is necessary because ground truth optical flow at event camera rates is unavailable. Answering this question establishes whether SNNs can move beyond simple classification tasks to tackle the challenging visual perception problems required for robotics.

Having established that SNNs can learn optical flow, the next challenge is deployment on actual neuromorphic hardware:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 2}}]
    How can a fully neuromorphic vision-to-control pipeline be realized on-board a flying robot?
\end{tcolorbox}

This question addresses the gap between networks trained on workstations with millions of parameters and the severe resource constraints of embedded neuromorphic processors. A complete pipeline must include not only perception but also the mapping from sensory estimates to control commands, all running within the neuron budget of chips like Loihi. Real-world flight tests provide the ultimate validation that the system works reliably in dynamic, unpredictable environments.

Even with successful deployment, training typically happens offline on powerful workstations. The third research question asks whether learning can happen during operation:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 3}}]
    How can a neural network learn to estimate depth from events on-board a flying robot in an online, self-supervised manner?
\end{tcolorbox}

Online learning during flight would enable robots to adapt to their specific operational environment, reducing the reality gap between training and deployment conditions. However, this requires dramatic improvements in computational efficiency: not just inference but the entire learning framework must run on-board in real-time. Depth estimation for obstacle avoidance serves as the task, combining the benefits of self-supervised learning with immediate practical utility for autonomous navigation.

Finally, having demonstrated learning, deployment, and online adaptation, the fourth question pushes toward ultimate sensor minimalism:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 4}}]
    How can vision alone replace inertial measurement units for flight attitude estimation and control?
\end{tcolorbox}

Eliminating the IMU would reduce sensor suite weight and power consumption---critical for insect-scale flying robots where every component counts. This question investigates whether event-based vision can extract not just dynamic motion information (like optical flow) but also quasi-static attitude information traditionally provided by accelerometers and gyroscopes. Success would demonstrate that event cameras can serve as the primary, or potentially only, sensor for autonomous flight.

Together, these four research questions trace a path from fundamental learning capability to practical hardware deployment to online adaptation to sensor minimalism. The following section outlines how each chapter of this thesis addresses one of these questions.

\section{Outline}

This thesis is structured around four main chapters, each addressing one of the research questions outlined above. The chapters build progressively: the first establishes that spiking neural networks can learn complex visual tasks, the second demonstrates deployment on neuromorphic hardware aboard a flying robot, the third enables online learning during flight, and the fourth achieves vision-only flight control by eliminating inertial sensors. Together, they trace a path from fundamental algorithmic capabilities to complete, deployed systems operating autonomously in the real world.

\textbf{Chapter~\ref{ch:neurips}: Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks.}
Prior to this work, spiking neural networks had been limited to relatively simple tasks such as classification on small-scale datasets like neuromorphic MNIST or DVS128 Gesture. Dense regression problems requiring per-pixel predictions---such as optical flow estimation---had remained firmly in the domain of traditional artificial neural networks. This chapter tackles the fundamental question of whether SNNs can be trained to solve such complex vision tasks in a self-supervised manner. The key challenge is that standard training approaches for ANNs do not transfer directly to SNNs due to the non-differentiable spiking function, sparse binary activity patterns, and more complex neuronal dynamics.

To enable SNN learning on this task, we introduce several critical modifications to the training pipeline. First, we modify the input event representation to encode much smaller time slices with minimal explicit temporal information, forcing the network's neuronal dynamics and recurrent connections to take responsibility for temporal integration. Second, we reformulate the contrast maximization loss function---which serves as the self-supervised training signal---to improve its convexity and make it more amenable to gradient-based optimization. Third, we systematically investigate elements of SNN training that have received limited attention, including parameter initialization strategies for sparse inputs, the shape and width of surrogate gradients, and the inclusion of adaptive neuronal mechanisms and learnable parameters.

The resulting training framework enables deep spiking neural networks to learn optical flow from event streams, matching the performance of state-of-the-art artificial neural networks on standard benchmarks like MVSEC. This demonstrates for the first time that SNNs can tackle complex, dense, real-world vision problems when provided with appropriate modifications to both the learning framework and network architecture. The work establishes the foundation for neuromorphic vision systems, proving that the capability exists---but it raises the next critical question: can such networks actually be deployed on resource-constrained neuromorphic hardware?

\textbf{Chapter~\ref{ch:sr}: Fully Neuromorphic Vision and Control for Autonomous Drone Flight.}
Having established that spiking neural networks can learn optical flow, this chapter addresses the deployment challenge: can a complete neuromorphic vision-to-control pipeline run on-board a flying robot? The gap between capability and deployment is substantial. Networks trained in Chapter~\ref{ch:neurips} contain hundreds of thousands to millions of parameters, while embedded neuromorphic processors like Intel's Loihi chip support only 262,144 neurons---orders of magnitude fewer. Moreover, a functioning autonomous system requires not just perception but also control: the network must not only estimate optical flow but also translate these estimates into appropriate motor commands for stable flight.

To fit within Loihi's stringent resource constraints, we dramatically reduce spatial resolution by processing only four regions of interest (ROIs) positioned at the corners of the visual field, each containing just 16Ã—16 pixels. The vision network, containing 28,800 neurons per ROI, estimates optical flow which is then decoded to ego-motion estimates using a linear mapping. Control commands are generated by a lightweight controller trained with evolutionary algorithms in simulation. This approach enables direct sim-to-real transfer: the controller learned in simulation works on the physical robot without further adaptation. The complete pipeline processes events asynchronously at 200 Hz while consuming only 7-12 milliwatts beyond Loihi's 0.94-watt idle power.

Real-world flight experiments demonstrate successful autonomous behaviors including hovering, landing, and lateral maneuvering---representing the first complex autonomous flight achieved with a fully neuromorphic pipeline. The extreme efficiency of the system validates the promise of neuromorphic computing for robotics: by matching the sparse, asynchronous nature of event cameras with spike-based processing, we achieve performance that would be impossible with conventional processors under the same power budget. However, a fundamental limitation remains: while the deployed system runs on-board, all training happens offline on powerful workstations. This raises the next challenge: can we enable learning to occur during the robot's operation?

\textbf{Chapter~\ref{ch:cvpr}: On-Device Self-Supervised Learning of Depth for Obstacle Avoidance.}
The previous chapters demonstrated that networks can be trained on event-based vision tasks and deployed on embedded hardware---but training and deployment remain separate phases. This chapter tackles a more ambitious goal: enabling self-supervised learning to run online, on-board a flying robot during operation. Online learning offers powerful advantages: it reduces the reality gap by training directly on data from the operational environment, allows adaptation to environment-specific conditions, and eliminates the need for large offline training datasets. However, making this practical requires dramatic improvements in computational efficiency. It is not sufficient to optimize only network inference; the entire learning framework---including loss computation, backpropagation, and parameter updates---must be efficient enough to run in real-time on resource-constrained platforms.

This chapter focuses on monocular depth estimation from events for obstacle avoidance, learned through self-supervised contrast maximization. We systematically optimize the training framework for both time and memory efficiency, introducing improvements that span data representation, network architecture, loss computation, and the optimization procedure itself. The resulting system uses a small recurrent convolutional network specifically designed for embedded deployment. Critically, we demonstrate that online learning during flight yields more accurate depth estimates and more successful obstacle avoidance compared to networks that are only pre-trained offline. This validates the core hypothesis: learning in the operational environment produces better performance than attempting to generalize from training data collected elsewhere.

The system achieves state-of-the-art performance among self-supervised event-based depth estimation methods on standard benchmarks, while being efficient enough to train on-board during flight. This closes a crucial loop: robots can now not only perceive and act, but also learn and adapt in their operational environments without external infrastructure. Having demonstrated learning, deployment, and online adaptation, one final question emerges: can we push even further toward sensor minimalism by eliminating traditional sensors?

\textbf{Chapter~\ref{ch:npjr}: Vision-Only Attitude Estimation Replacing Inertial Measurement Units.}
The progression through the previous chapters---from learning to deployment to online adaptation---has established that event-based vision can serve as a powerful sensor for flying robots. This final chapter pushes toward the ultimate minimalism: can vision alone replace inertial measurement units for flight control? IMUs, which provide measurements of acceleration and angular velocity, are ubiquitous in flying robots and are typically considered indispensable for stable flight. However, for insect-scale UAVs where every sensor adds weight and power consumption, eliminating the IMU could enable even lighter and more efficient designs. Biological inspiration supports this possibility: flying insects exhibit remarkable flight agility without any known dedicated gravity sensor, and honeybees even lack the halteres (motion sensors) found in other flying insects.

Prior work has shown that attitude estimation from vision is theoretically possible by combining optical flow with motion models, but hardware limitations and computational constraints have prevented practical implementation on flying robots. This chapter demonstrates the first fully on-board, vision-only attitude control system using event cameras. A small recurrent convolutional neural network learns to estimate both attitude (roll, pitch, and yaw angles) and angular rotation rates directly from event streams. Unlike the self-supervised approaches in previous chapters, here we employ supervised learning: the IMU provides ground truth labels during a training phase, after which the IMU can be removed and the network takes over its function.

Real-world flight tests validate that the network's estimates are accurate enough to completely replace the IMU in the flight control loop, enabling stable hovering and maneuvering with vision as the only sensor. Systematic investigations reveal interesting properties of the learned representations: while larger receptive fields and wider fields of view improve absolute accuracy during training, narrower field-of-view networks generalize better across different environments. This suggests the networks learn internal motion models that compensate for the limited visual information. This work completes the four-chapter arc of this thesis, demonstrating that event-based vision can serve not just as a supplement to other sensors but as the primary---and potentially only---sensor necessary for autonomous flight. It brings the vision of insect-scale flying robots with minimal sensor suites one step closer to reality.

Together, these four chapters demonstrate a complete pathway from fundamental learning capabilities to deployed, adaptive, minimal-sensor systems. The thesis shows that the promise of neuromorphic event-based vision for robotics is not merely theoretical: with appropriate co-design of algorithms, learning frameworks, and hardware deployment strategies, fully autonomous neuromorphic flight is achievable today.
