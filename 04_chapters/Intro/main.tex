\chapter{Introduction}\label{cha:intro}

% =============================================================================
% BACKGROUND (~1500 words)
% =============================================================================

% --- Act 1: The Autonomous Robot Revolution (~350 words) ---
\todo{Add visual abstract (overview)}

\dropcap{A}{utonomous} robots can transform how we live, work, and interact with our environment. Delivery robots could reshape urban logistics, reducing traffic congestion while enabling rapid, sustainable last-mile transport. Agricultural robots offer the potential to monitor crops with unprecedented precision, optimizing yields while minimizing resource consumption. In search-and-rescue operations, autonomous robots could locate survivors in disaster zones far faster than human teams, operating in conditions too dangerous for responders. Environmental monitoring robots could track wildlife populations, measure air quality, and survey ecosystems at scales previously impossible~\cite{decroon2022insectinspired}.

For these visions to become reality---and to become forces for good---autonomous robots must meet three essential criteria. They must be \emph{capable}: able to perceive and navigate complex, unpredictable environments, and interact with the world in a useful manner. They must be \emph{safe}: equipped with perception systems robust to changes and failures, and minimal in size and weight to limit physical risk to humans. And they must be \emph{efficient}: able to operate for extended periods without the heavy batteries and powerful processors that currently limit their reach and utility.

Current artificial intelligence falls short on all these criteria. State-of-the-art autonomous systems achieve remarkable capabilities---consider the recent demonstration of AI defeating human champions in drone racing~\cite{kaufmann2023championlevel}---but at substantial computational cost. In the case of flying robots, modern vision systems typically require graphics processing units consuming tens of watts of power, imposing hundreds of grams of payload that directly limit flight time and agility~\cite{nvidia}. For small drones that weigh only tens of grams, such computational requirements are simply infeasible. This creates a fundamental tension: how can we achieve the perceptual capabilities required for safe, useful autonomy while respecting the severe energy constraints of lightweight robotic platforms?

% --- Act 2: Nature's Efficiency Masterclass (~400 words) ---

Nature offers a compelling answer. Biological systems demonstrate that sophisticated perception and agile navigation need not require massive computational resources. The honeybee navigates complex environments, avoids obstacles, and finds its way home over kilometers of terrain---all with a brain containing fewer than one million neurons~\cite{menzel2001cognitive}. This is not an isolated example: the natural world is full of creatures that perform remarkable computational feats within extreme resource constraints.

% The fruit fly \emph{Drosophila melanogaster} possesses motion detection circuits of extraordinary sophistication. Its visual system responds to motion with temporal tuning optimized for the statistics of natural scenes, enabling precise flight control despite having a brain smaller than a pinhead~\cite{arenz2017temporal}. Desert ants navigate featureless terrain for hundreds of meters, then return directly to their nests using visual route memories that encode scene familiarity rather than explicit maps~\cite{baddeley2012model}. Dragonflies intercept flying prey with millisecond precision, predicting interception trajectories using internal models computed by neural circuits comprising only hundreds of neurons~\cite{vos2024hunt}. These examples span different sensory modalities and computational strategies, yet share a common principle: biological intelligence achieves remarkable capabilities through sparse, efficient computation.

What makes biological neural systems so efficient? The answer lies in how they process information. Unlike conventional computers that shuttle data continuously between memory and processors, biological neurons communicate through discrete \emph{spikes}: brief electrical pulses that carry information through their timing rather than continuous voltage levels~\cite{sterling2015principles}. Neurons fire only when they have something to say, remaining silent otherwise. This sparse, event-driven computation means that energy is expended only when and where information needs to be processed. The brain does not render complete images or process every pixel; instead, it responds selectively to changes and salient features in the visual field.

This insight suggests a different approach to robot vision: rather than attempting to compress conventional deep learning onto resource-constrained platforms, we might instead adopt the computational principles that evolution has refined over millions of years. What if we could build artificial systems that sense and process information in the same sparse, event-driven manner as biological brains?

% --- Act 3: The Neuromorphic Solution (~400 words) ---

Recent technological advances have made this vision increasingly feasible. Event cameras represent a fundamental departure from conventional imaging, one that aligns closely with biological principles. Unlike traditional cameras that capture complete frames at fixed intervals (typically 30-60 times per second), event cameras operate asynchronously, with each pixel independently detecting brightness changes and generating events at microsecond resolution~\cite{gallego2020eventbased}. The design is directly inspired by the retina, where photoreceptors respond to changes in light intensity rather than absolute brightness levels~\cite{posch2014retinomorphic}. This ``retinomorphic'' sensing paradigm offers several key advantages for robotics: temporal resolution three orders of magnitude higher than standard cameras, power consumption measured in milliwatts, and sparse output that naturally compresses visual information to only the changing parts of the scene.

The \emph{neuromorphic} paradigm extends beyond sensing to computation. Neuromorphic processors, inspired by the structure and function of biological neural networks, process information through spiking neural networks (SNNs) that communicate via discrete spikes rather than continuous values. Like biological neurons, neuromorphic chips perform sparse, event-driven computation, promising energy consumption orders of magnitude lower than conventional processors~\cite{davies2021advancing,merolla2014million}. State-of-the-art neuromorphic processors such as Intel's Loihi~\cite{davies2018loihi} and IBM's TrueNorth~\cite{merolla2014million} operate in the milliwatt range while supporting hundreds of thousands of neurons.

When coupled together, event cameras and neuromorphic processors could enable a fully asynchronous, event-driven vision pipeline. Individual brightness changes would trigger spikes that propagate through the network without any frame-based accumulation or synchronous processing. Such a pipeline would match the sparse and asynchronous nature of both biological sensing and neural computation, potentially achieving the efficiency that nature demonstrates while maintaining the capabilities that autonomous robots require~\cite{sandamirskaya2022neuromorphic}. The promise is compelling: robot vision systems that approach the efficiency of insect brains while running on practical, deployable hardware.

% --- Act 4: The Gap Between Promise and Practice (~350 words) ---

Despite this promise, significant challenges have prevented the widespread adoption of neuromorphic vision for robotics. Training spiking neural networks remains substantially more difficult than training conventional artificial neural networks. Surrogate gradient methods~\cite{neftci2019surrogate, zenke2021remarkable} have enabled backpropagation through SNNs, but additional challenges arise from sparse, binary spike activity that can lead to vanishing gradients, as well as complex neuronal dynamics requiring careful initialization~\cite{pfeiffer2018deep,tavanaei2019deep}. As a consequence, SNNs have largely been limited to relatively simple tasks such as classification on small datasets like neuromorphic MNIST or DVS128 Gesture~\cite{amir2017low}. Dense regression problems, such as optical flow estimation, depth prediction and pose estimation, have remained firmly in the domain of conventional deep learning.

Hardware constraints compound these algorithmic challenges. Even state-of-the-art neuromorphic chips support only hundreds of thousands of neurons, orders of magnitude fewer than the millions typically required by deep vision networks. This forces severe compromises in network capacity that must be carefully managed through algorithmic innovation.

A third challenge is training data. For event cameras operating at microsecond temporal resolution, obtaining ground truth labels at comparable rates is impractical. Motion capture systems and depth sensors typically operate at only 10-20 Hz, creating a fundamental mismatch between sensor capabilities and supervision signals. This has motivated self-supervised learning approaches that can learn directly from event streams without external ground truth~\cite{gallego2018unifying,zhu2019unsupervised}, but such frameworks are computationally expensive, challenging to run on embedded platforms.

Finally, a gap remains between laboratory demonstrations and deployed autonomous systems. Most research evaluates on recorded datasets, but practical deployment demands robustness to varying conditions, real-time performance guarantees, and integration with complete perception-to-action pipelines. These integration challenges are substantial and often underestimated.

This dissertation addresses these gaps through a systematic progression of contributions, demonstrating that neuromorphic event-based vision can move from theoretical promise to practical deployment on autonomous flying robots.

% =============================================================================
% PROBLEM STATEMENT AND RESEARCH QUESTIONS (~1000 words)
% =============================================================================

\section{Problem statement and research questions}

The challenges outlined above motivate the central research question of this dissertation:

\tcbset{colframe=black!90!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Main Research Question}}]
    How can robots learn to perceive the world with event-based vision in a way that enables effective acting in that world?
\end{tcolorbox}

This question emphasizes that solutions must not only achieve high perceptual accuracy but also be deployable with low enough latency and computational requirements for real-time robot control. To make this challenge concrete, we focus on the specific domain of autonomous flight:

\tcbset{colframe=black!90!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Problem Statement}}]
    How can a flying robot learn to navigate autonomously when it only has event-based vision?
\end{tcolorbox}

Flying robots with just event-based vision represent an extreme test case: they combine the strictest resource constraints with the highest demands for low-latency, robust perception. Addressing this problem requires tackling challenges across the full stack, from learning algorithms to hardware deployment to online adaptation. We decompose this overarching problem into four specific research questions, each corresponding to one main chapter of this thesis. Together, these questions trace a path toward the ultimate goal: a fully autonomous, fully neuromorphic flying robot.

\subsection*{Self-supervised learning for spiking neural networks}

The first fundamental question is whether spiking neural networks can learn complex visual tasks at all. Prior to this work, SNNs had been limited to simple classification tasks on small-scale datasets. Dense regression problems, requiring per-pixel predictions with sophisticated temporal integration, had remained out of reach.

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 1}}]
    How can spiking neural networks learn to estimate dense optical flow from event streams in a self-supervised manner?
\end{tcolorbox}

Optical flow estimation serves as an ideal test case: it requires dense predictions across the entire visual field, temporal integration of motion information, and works without ground truth when learned self-supervised. Self-supervision is necessary because ground truth optical flow at event camera rates is unavailable. Success here would establish that SNNs can move beyond toy problems to tackle the challenging visual perception required for robotics.

\subsection*{Neuromorphic deployment on flying robots}

Having established learning capability, the next challenge is deployment on actual neuromorphic hardware aboard a resource-constrained robot:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 2}}]
    How can a fully neuromorphic vision-to-control pipeline be realized on-board a flying robot?
\end{tcolorbox}

This question addresses the gap between networks trained on workstations and the severe constraints of embedded neuromorphic processors. Networks from Research Question~1 contain hundreds of thousands of parameters, but chips like Intel's Loihi support only \textasciitilde{}260,000 neurons. Moreover, a complete autonomous system requires not only perception but also control: translating visual estimates into motor commands for stable flight. Real-world flight tests provide the ultimate validation that the system works reliably in dynamic, unpredictable environments.

\subsection*{Online self-supervised learning during flight}

Even with successful deployment, training typically happens offline on powerful workstations. The third research question asks whether learning can happen during operation:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 3}}]
    How can a neural network learn to estimate depth from events on-board a flying robot in an online, self-supervised manner?
\end{tcolorbox}

Online learning during flight would enable robots to adapt to their specific operational environment, eliminating the reality gap between training and deployment conditions. However, this requires dramatic efficiency improvements: not just inference but the entire learning framework (loss computation, backpropagation, parameter updates) must run on-board in real-time. Depth estimation for obstacle avoidance provides the task, combining self-supervised learning with immediate practical utility for autonomous navigation.

\subsection*{Vision-only flight control}

Finally, having demonstrated learning, deployment, and online adaptation, the fourth question pushes toward ultimate sensor minimalism:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}[title={\textbf{Research Question 4}}]
    How can vision alone replace inertial measurement units for flight attitude estimation and control?
\end{tcolorbox}

Inertial measurement units (IMUs) are ubiquitous in flying robots, providing measurements of acceleration and angular velocity considered essential for stable flight. Yet for insect-scale robots, where a sensor weighing milligrams and consuming microwatts can represent a significant fraction of the total budget~\cite{yu2025tinysense}, eliminating the IMU would be transformative. Biological inspiration supports this possibility: flying insects achieve remarkable agility without dedicated inertial sensors. Success would demonstrate that event cameras can serve as the primary, or potentially only, sensor for autonomous flight.

% =============================================================================
% SCOPE AND LIMITATIONS (~200 words)
% =============================================================================

\section{Scope and limitations}

This dissertation focuses on flying robots as the primary platform for investigating event-based neuromorphic vision. Flying robots, particularly small drones, represent the most constrained application domain: they face extreme limitations in size, weight, and power (SWaP), where every gram of payload and every milliwatt of power consumption directly affects flight time and maneuverability. Additionally, flight control demands high-frequency perception (typically exceeding 100~Hz) to maintain stability. These stringent constraints make flying robots an ideal testbed: solutions that work under such demanding conditions can transfer to less constrained platforms.

The research presented here focuses specifically on ego-motion perception (optical flow, depth, and attitude estimation) rather than higher-level scene understanding tasks such as object recognition or semantic segmentation. The flying robot experiments assume relatively textured environments that provide sufficient visual features for the event camera. Finally, while the neuromorphic hardware deployment demonstrates practical feasibility, neuromorphic hardware remains in early stages of development; the specific constraints of current processors influence the network designs presented, and limited interfacing prevents efficiency advantages at the system level. These limitations define the boundaries within which the contributions of this thesis should be interpreted.

% =============================================================================
% OUTLINE (~400 words)
% =============================================================================

\section{Outline}

This thesis is structured around four main chapters, each addressing one research question. The chapters trace a progression from fundamental learning capability to practical deployment, online adaptation, and ultimate sensor minimalism, hence building toward the goal of fully autonomous, fully neuromorphic flying robots.

\cref{ch:neurips} establishes that SNNs can learn complex visual tasks. We introduce modifications to input encoding, loss formulation, and training procedures that enable deep spiking networks to learn optical flow from events self-supervised, matching state-of-the-art conventional networks on standard benchmarks. This demonstrates fundamental SNN capability for dense, real-world vision problems. \cref{ch:sr} demonstrates deployment on Intel's Loihi neuromorphic processor aboard a flying drone. By dramatically reducing spatial resolution and training a lightweight evolutionary controller, we achieve the first complex autonomous flight with a fully neuromorphic pipeline, operating at 200~Hz while consuming only 7-12~mW beyond idle power. \cref{ch:cvpr} enables self-supervised learning to run online, on-board during flight. We systematically optimize the contrast maximization framework for time and memory efficiency, demonstrating that online learning yields more accurate depth estimates and better obstacle avoidance than offline pre-training alone. \cref{ch:npjr} achieves ultimate sensor minimalism by eliminating the IMU. A recurrent neural network learns to estimate attitude and rotation rates from events alone, enabling stable flight with vision as the only sensor. This brings insect-scale flying robots with minimal sensor suites closer to reality.

To conclude, \cref{ch:conclusion} summarizes this thesis' main contributions, discusses their implications, and provides directions for future research.

% Placeholder for future visual overview figure
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{04_chapters/Intro/figures/overview.pdf}
%     \caption{Visual overview of thesis structure. Each chapter builds toward fully autonomous, fully neuromorphic flight: establishing SNN learning capability (Chapter~\ref{ch:neurips}), demonstrating neuromorphic deployment (Chapter~\ref{ch:sr}), enabling online adaptation (Chapter~\ref{ch:cvpr}), and achieving vision-only control (Chapter~\ref{ch:npjr}).}
%     \label{fig:intro_overview}
% \end{figure}
