\section{Related work}
\label{sec:cvpr_related}

\paragraph{SSL of monocular depth.} Self-supervised learning of monocular depth has garnered significant attention since the early works that focused on joint depth-pose estimation for static scenes~\cite{zhou2017unsupervised, vijayanarasimhan2017sfmnet, godard2019digging}. These foundational studies have spurred further advancements to handle more complex scenarios, such as dynamic scenes, by integrating optical flow estimation~\cite{ranjan2019competitive, yin2018geonet}, leveraging regularization techniques~\cite{li2021unsupervised}, or incorporating motion segmentation~\cite{sun2023dynamodepth}. Additionally, several works have explored learning camera parameters~\cite{gordon2019depth, chen2019selfsupervised}, which is particularly relevant for on-device learning scenarios with unknown cameras.

Wang~\textit{et al.}~\cite{wang2019recurrent} demonstrate that recurrent networks can enhance depth estimation by effectively utilizing information from multiple frames, resulting in more consistent depth scale predictions. Similarly, Bian~\textit{et al.}~\cite{bian2019unsupervised} introduce a loss term to encourage scale consistency, addressing a critical challenge in depth estimation. Achieving depth predictions with a consistent scale significantly enhances the stability of robot control systems relying on these depth estimates. By combining recurrent architectures and scale-consistent training, our approach aligns with these advancements, offering a robust solution for on-device learning during real-world operation.

\paragraph{SSL through contrast maximization.} The contrast maximization framework enables the extraction of accurate optical flow information by leveraging the temporal misalignment of accumulated events~\cite{gallego2019focus, gallego2018unifying}. This optical flow can be estimated either through model-based methods~\cite{shiba2024secrets} or using neural networks~\cite{zhu2019unsupervised, hagenaars2021selfsupervised, paredes-valles2023taming}. The choice of the optical flow model itself offers a spectrum of possibilities, ranging from linear models~\cite{zhu2019unsupervised, hagenaars2021selfsupervised}, to segmented representations~\cite{paredes-valles2023taming}, and even parametrized trajectories~\cite{hamann2025motionprior}.

In our work, we adopt the approach outlined in~\cite{paredes-valles2023taming}, as it aligns with our goal of achieving high-frequency estimation using a neural network. Moreover, this approach supports efficient inference, which is critical for real-time applications such as robotic navigation or on-device learning. The ability to accommodate nonlinear event trajectories further broadens its applicability, making it a robust choice for extracting motion information in challenging conditions where traditional linear assumptions might fail.

\paragraph{SSL of depth from events.} Early works focused on jointly estimating depth and pose directly from events, employing either contrast maximization techniques~\cite{zhu2019unsupervised} or photometric error methods based on event frames~\cite{ye2020unsupervised}. A notable contribution by Zhu~\textit{et al.}~\cite{zhu2019unsupervised} was their ability to estimate metric depth through the incorporation of a stereo loss term, enabling absolute depth recovery. These methods demonstrated the potential of event-based sensing for depth and pose estimation in static scenes.

More recent research has expanded these approaches to address dynamic scenes~\cite{georgoulis2024out}, where traditional static-scene assumptions do not hold, and developed more principled frameworks for model-based contrast maximization to jointly estimate depth, ego-motion, and optical flow~\cite{shiba2024secrets}. These advancements represent a significant step toward estimating complex, real-world motion using event data.

Additionally, some works have explored the integration of intensity images as either inputs or components of the loss function~\cite{zhu2023selfsupervised}. This hybrid approach leverages the complementary information provided by intensity images to enhance the performance of event-based depth estimation, especially in scenarios where pure event data might lack sufficient structure or texture information.

\paragraph{Learning depth for drones.} Already in 2016, Lamers~\textit{et al.}~\cite{lamers2016selfsupervised} demonstrated the feasibility of learning depth estimation on board a small flapping-wing drone. Although their approach did not produce dense depth maps, it proved effective for navigation, marking an early milestone in on-device learning for aerial robotics. Several works~\cite{pirvu2021depth,licaret2022ufo} combine unsupervised depth learning with an analytic odometry-flow pipeline (from external sources) to achieve metric monocular depth. While their networks are lightweight enough for embedded hardware, they are not actually used on drones.

Recent works have focused on generating dense depth maps on board drones. Liu~\textit{et al.}~\cite{liu2023nano} developed a system to estimate depth from images for obstacle avoidance on a tiny quadrotor, leveraging recorded real-world datasets for training. Bhattacharya~\textit{et al.}~\cite{bhattacharya2024monocular}, on the other hand, used a simulation-based approach to train depth estimation from events. They successfully transitioned to real-world applications by performing offline fine-tuning on real-world data, enabling effective obstacle avoidance in practice.

In this work, we take a distinct approach by performing fine-tuning on board the drone in an online fashion during flight, adapting the model in real time while actively avoiding obstacles. This method combines the strengths of self-supervised learning with real-time adaptability, paving the way for more robust and efficient systems capable of handling dynamic environments. By eliminating the reliance on extensive offline fine-tuning or pre-collected datasets, our approach addresses the challenges of real-world deployment more directly.
