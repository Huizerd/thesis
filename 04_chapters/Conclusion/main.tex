\chapter{Conclusion}
\label{cha:conclusion}

\dropcap{I}{n} this concluding chapter, we revisit and address the research questions and problem statement presented in \chapref{cha:intro}, which have structured the work conducted throughout this dissertation. We then discuss the broader implications of our findings and the potential impact that they may have on the broader scientific research field. Finally, we explore several potential avenues for future research.

\section{Answers to research questions}

The first research question derived from our problem statement was formulated as follows:

\tcbset{colframe=black!60!, coltitle=white}
\begin{tcolorbox}
	\textbf{RQ1}: How can fast, autonomous flight through gates be achieved with frame-based perception and conventional processing in a GPS-denied environment?
\end{tcolorbox}

This question was addressed in \chapref{cha:fr} with the development of a robust and efficient vision-based navigation solution for autonomous drone racing. To enable fast and agile flight using conventional sensing and processing, a lightweight monocular pipeline was devised, focusing exclusively on gate information. This was achieved by leveraging the fact that the gates were the only objects in the environment whose approximate appearance, location, and orientation were known a priori. The first step in the proposed pipeline is to detect the corners of the next gate to be traversed according to the flight plan, in the image space. This is done by first segmenting gate pixels using an artificial neural network (ANN) and then searching for the corners in the resulting mask using a light active-vision algorithm. The corners that can be validated using the robot's prior expectations of the gate's appearance and geometry are then used to estimate the pose of the camera in the world frame with a perspective-n-point algorithm. These vision measurements are enhanced with model-based predictions through random sample consensus, resulting in the state estimates used to control the robot. To finalize, a risk-aware control strategy is employed to balance the trade-off between speed and safety. The proposed solution was validated in hardware-in-the-loop simulation and real-world experiments. 
In fact, it was benchmarked against other state-of-the-art visual-inertial navigation solutions in the first Artificial Intelligence Robotic Racing season in 2019, where it was the fastest and most robust approach in those conditions, with the runner-up team being (only) three seconds slower in the final race.
%In fact, it was benchmarked against other state-of-the-art visual-inertial navigation solutions in the first Artificial Intelligence Robotic Racing season in 2019, where it was the fastest and most robust approach in those conditions, with the runner-up team being (only) 25\% slower on average.
%In fact, it was benchmarked against other state-of-the-art visual-inertial navigation solutions in the first Artificial Intelligence Robotic Racing season in 2019, where it was the fastest and most robust in those conditions, where it demonstrated its superiority as the fastest and most robust approach to autonomous drone racing in those conditions. 
Significantly, this chapter highlights the importance of minimizing latency in the perception pipeline to achieve high-speed autonomous flight. Our solution prioritized real-time performance at the system's maximum operating frequency (i.e., 60 Hz) over the use of more complex and computationally expensive algorithms.

\begin{tcolorbox}
	\textbf{RQ2}: How can we leverage the knowledge of the inner working of event cameras to learn event-based frame reconstruction in a self-supervised fashion?
\end{tcolorbox}

This second research question was motivated by the limited adoption of event cameras in robotics, despite their numerous advantages. This limited adoption is primarily due to the lack of a mature algorithmic ecosystem capable of effectively utilizing the sparse and asynchronous nature of event camera output. In \chapref{cha:cvpr}, we addressed this question by proposing a novel self-supervised learning (SSL) framework for event-based frame reconstruction. Our proposed solution is based on the event generative model, which, under constant illumination, establishes a relationship between events, brightness, and optical flow. Specifically, we demonstrate that this model can be leveraged to learn to reconstruct brightness frames from the events without relying on ground-truth data, as long as the optical flow encoded in the events is known or can be estimated. To achieve this, we train ANNs to minimize the discrepancy between the input events and the model-based predictions. The effectiveness of our training pipeline was validated using multiple datasets, where our networks demonstrated performance comparable to the state-of-the-art methods, despite not having access to reference frames during the training process. Additionally, we addressed the task of event-based optical flow estimation within the SSL framework. Our approach utilized the concept of contrast maximization for motion compensation, allowing us to learn event-based optical flow directly from the input events. Furthermore, we proposed a lightweight neural network architecture for event-based optical flow, which achieved high-speed inference while maintaining a minimal decrease in performance.

\begin{tcolorbox}
	\textbf{RQ3}: How can a spiking neural network learn to develop event-based motion selectivity in an unsupervised fashion?
\end{tcolorbox}

This third research question, focusing on event-based optical flow estimation with spiking neural networks (SNNs), was addressed in \chapref{cha:tpami}. Here, we introduced the first SNN that develops selectivity to motion, including direction and speed, in an unsupervised manner from the input event stream. The success of our approach was facilitated by the development of several key components. Firstly, we proposed a spiking neuron model capable of effectively handling the rapidly varying input statistics of event cameras through pre-synaptic adaptation. Secondly, we formulated a novel version of the correlation-based spike-timing-dependent plasticity (STDP) rule, which differs from the existing state-of-the-art approaches by being inherently stable. And finally, we designed a convolutional SNN architecture that learns to perform hierarchical feature extraction. Specifically, it starts by extracting geometric features, followed by capturing their local motion using multi-synaptic connections with different temporal delays, and eventually inferring global motion estimates via spatial integration. The effectiveness of this approach was validated through experimentation using both synthetic and real event sequences. However, due to the absence of supervision, quantitative comparisons with the state-of-the-art methods posed challenges. As a result, we relied on extensive qualitative analysis to assess and compare the performance of our approach. Most significantly, this chapter highlights the potential of SNNs to perform low-latency, event-based optical flow estimation. %Instead of having to accumulate events in the input representations, our approach allows for the processing of events nearly as they are generated by the sensor, with the integration of spatiotemporal information happening within the model itself.

\begin{tcolorbox}
	\textbf{RQ4}: How can low-latency, event-based optical flow be learned in a self-supervised fashion with spiking neural networks?
\end{tcolorbox}

This fourth research question, driven by the limitations of unsupervised learning, was addressed in \chapreftwo{cha:neurips}{cha:iccv}. In \chapref{cha:neurips}, we presented the first set of deep SNNs to successfully solve the problem of event-based optical flow estimation. To accomplish this, we reformulated the state-of-the-art training pipeline for ANNs (i.e., the aforementioned contrast maximization) to significantly reduce the time windows presented to the networks. Additionally, we refined the SSL loss function to enhance its convexity. Prior to training with our framework, we augmented various ANN architectures from literature with explicit and/or implicit recurrency, alongside the incorporation of the spiking behavior. Extensive quantitative and qualitative evaluations were conducted using multiple datasets. Our results not only confirm the efficacy of our training pipeline, but also demonstrate that the proposed set of recurrent ANNs and SNNs perform comparably to the state-of-the-art self-supervised methods.

Despite the significant accomplishments of \chapref{cha:neurips}, the proposed training pipeline assumes linear motion of events within the timeframe of their loss function, limiting its ability to accurately capture the true trajectory of scene points over time. To overcome this, \chapref{cha:iccv} introduces a reformulated pipeline that addresses the scalability to high inference frequencies while accurately capturing the true trajectory of scene points. An iterative event warping module and a multi-timescale loss function are the main additions to the pipeline. The former unlocks a novel multi-reference loss function that improves the accuracy of the predicted optical flow, while the latter enhances the robustness of the training process. The effectiveness of this new approach was validated using multiple datasets, where our models demonstrated superior performance compared to both the self-supervised and model-based baselines, surpassing them by significant margins. Please note that, although this reformulation of the training pipeline was validated with ANNs, it is extrapolable to SNNs as well.

\begin{tcolorbox}
	\textbf{RQ5}: How can a spiking neural network be trained in a self-supervised fashion to perform event-based optical flow estimation while running on a neuromorphic processor in the control loop of an autonomous flying robot?
\end{tcolorbox}

This fifth and last research question was tackled in \chapref{cha:sr}, where we introduced the groundbreaking concept of a fully neuromorphic vision-to-control pipeline for controlling a freely flying robot. The experimental setup involved equipping the robot with a downward-facing event camera, which captured data from a static planar surface, and a specialized neuromorphic processor. The latter was used to run a compact SNN that was trained to process high-dimensional raw event-camera data and output low-level control actions. This allowed for autonomous vision-based ego-motion estimation and control at approximately 200 Hz, spending only 27 $\mu$J per network inference. The proposed learning setup effectively addresses the challenge of slow and inaccurate simulation of event-based data, as it allows for the independent training of vision and control. While the vision part of the network is trained using an adapted version of the self-supervised pipelines from \chapreftwo{cha:neurips}{cha:iccv}, the control policy is learned through evolution in simulation without the need to simulate events. Real-world experiments were conducted, wherein the event camera and neuromorphic processor were integrated into the control loop of the flying robot. The results showcased the effectiveness of our approach, as the robot accurately followed various ego-motion setpoints and successfully performed hovering, landing, lateral maneuvers, and even constant yaw rate control.

The answer to this final research question, which builds upon the contributions of the previous chapters, also serves as the answer to our initial problem statement, which was formulated as follows:

\tcbset{colframe=black!90!, coltitle=white}
\begin{tcolorbox}
	\textbf{Problem Statement}: How can optical-flow-based autonomous navigation be realized with an event-based camera and a neuromorphic processor in the control loop of a flying robot?
\end{tcolorbox}

Our studies demonstrate the benefits of incorporating neuromorphic technology into the vision-based state estimation pipeline of autonomous flying robots, particularly in terms of latency and power consumption. The results obtained from our experiments are highly encouraging and suggest that the integration of neuromorphic sensing and processing could make deep neural networks more accessible for small autonomous robots (and other edge devices with limited resources). This advancement has the potential to enhance the agility, versatility, and robustness of these robots, bringing them closer to the capabilities exhibited by flying insects.

% \tcbset{colframe=black!90!, coltitle=white}
% \begin{tcolorbox}
	% 	\textbf{Driving Research Question}: How can neuromorphic perception and processing be incorporated into the vision-based, state-estimation pipeline of an autonomous flying robot?
	% \end{tcolorbox}

\section{Discussion}

The pursuit of incorporating neuromorphic technology into the control loop of autonomous flying robots has been the driving force behind the research presented in this dissertation. In this section, we delve into the main challenges encountered during this journey and the lessons we have gleaned along the way. Additionally, we shed light on the wider implications of our findings and the potential impact they can have on the broader scientific research field.

\subsection*{Efficient intelligence for flying robots}

Flying robots face inherent limitations due to their restricted payload capacity and power budget. Throughout our research, we have recognized the critical significance of developing efficient perception and processing pipelines to enable the autonomy of these robots. This theme has been at the core of our work, driving our exploration and innovations. In \chapref{cha:fr}, we developed a vision-based pipeline specifically tailored for autonomous drone racing with frame-based cameras. To ensure fast, agile, and robust flight, we leveraged classical (lightweight) algorithms wherever possible, while ANNs were used selectively where needed. Our research then shifted to exploring event-based cameras and SNNs as alternatives to conventional sensing and processing in \chapreffour{cha:cvpr}{cha:tpami}{cha:neurips}{cha:iccv}. These chapters highlight the potential of neuromorphic computing to achieve low-latency, low-power vision-based perception for robotics. Finally, in \chapref{cha:sr}, we demonstrated the feasibility of a fully neuromorphic vision-based pipeline for controlling a freely flying robot. This solution, which is characterized by an energy consumption in the order of microjoules and a latency of milliseconds, represents a significant step towards developing efficient intelligence for flying robots, as it shows that heavy processing can be realized on-board with only a fraction of the power needed to fly.  

\subsection*{Low-latency processing of event data with SNNs}

The event-camera literature has primarily focused on the use of stateless ANNs to process the data, with only a limited number of studies exploring the use of SNNs in often less complicated tasks. In this dissertation, we argue that to realize the full potential of event cameras and achieve low-latency and low-power solutions, stateful SNNs should process the incoming events nearly as they are generated by the sensor, with no accumulation in between. To support this claim, we have demonstrated the training of architecturally-complex SNNs for real-world, large-scale problems, specifically event-based optical flow estimation. In \chapref{cha:tpami}, we approached the problem from an unsupervised learning perspective using STDP to train the SNNs, while in \chapreftwo{cha:neurips}{cha:iccv}, we explored the self-supervised learning paradigm by employing new formulations of contrast maximization for motion compensation. In both approaches, we shortened the time windows presented to the networks and removed the temporal information from the input representations, approximating the way in which SNNs would receive events directly from the camera. This approach, which promotes the integration of spatiotemporal information within the models themselves through their internal dynamics, was then employed in \chapref{cha:sr} to realize the fully neuromorphic autonomous flight of a robot. Note that this idea has already sparked significant interest in the event-based optical flow literature, and several subsequent works have followed our findings \cite{ding2022spatio, ponghiran2022event, wu2022lightweight}, as they can potentially lead to lightweight solutions that are also robust to large pixel displacements.

\subsection*{Training SNNs with and without supervision}

In this dissertation, we tackled the problem of training SNNs for event-based optical flow estimation from two different learning perspectives: unsupervised and self-supervised. In \chapref{cha:tpami}, our focus was on unsupervised learning using the STDP rule, which adjusts the synaptic weights based on the temporal correlation between pre- and postsynaptic spikes. Despite the simplicity of this local learning rule, we successfully solved the task at hand by leveraging our knowledge of optical flow and the characteristics of event cameras to design an SNN capable of developing motion selectivity using this learning rule. However, the lack of supervision in STDP presents challenges. Firstly, it is difficult to directly benchmark our approach against the state-of-the-art since the learned features have limited controllability (i.e., we cannot specify what the network should learn). Secondly, careful fine-tuning of hyperparameters such as firing thresholds, decays, and synaptic delays is required to maintain a balanced network activity. On the other hand, the SSL paradigm explored in \chaprefthree{cha:neurips}{cha:iccv}{cha:sr} involves defining a loss function to be minimized during training, using the well-known backpropagation (through time) algorithm. This approach, once compensated for the non-differentiability of the spiking activation function \cite{neftciSurrogateGradientLearning2019, zenkeRemarkableRobustnessSurrogate2021}, enables us to leverage the vast array of tools and techniques developed for training ANNs. In these chapters, we demonstrated the effectiveness of this approach by training SNNs capable of estimating event-based optical flow with accuracy levels comparable to those of their ANN counterparts, and with the added benefit of being able to run on a neuromorphic processor. Note that the findings from our investigations on estimating event-based optical flow with SNNs have gotten the attention of the broader research community, as evidenced by the numerous follow-up works \cite{lee2020spike, vigneron2020critical, chaney2021self, barbier2021spike, liu2021event, parameshwaraSpikeMSDeepSpiking2021, she2021speed, peveri2021cortically, che2022differentiable, di2022kraken, yu2022improving, kosta2022adaptive, chamand2022self, zou2023event, negi2023best, iturbe2023nimbleai}.

\section{Outlook}

In this section, we explore several potential avenues for future research, which we believe can build upon the findings presented in this dissertation.

\subsection*{Toward per-event processing with SNNs}

One of the key insights from our research is that to fully harness the potential of event-based solutions, SNNs need to be trained to process incoming events nearly instantaneously as they are generated by the sensor. However, achieving this per-event processing approach is more complex than it might appear. There are two main challenges associated with this goal. Firstly, SNNs are recurrent networks with intricate internal dynamics. When the simulation timestep is decreased to enable per-event processing, the training process becomes significantly slower, and the memory requirements increase drastically if training with backpropagation through time (BPTT). BPTT, which has been shown in this dissertation to be a robust and effective gradient-based learning rule for SNNs, relies on accessing the network's internal states from previous timesteps to perform credit assignment over time. Consequently, these states need to be stored in memory throughout the BPTT process, which adds to the memory requirements. Secondly, as the simulation timestep decreases, the sparsity of the input increases. This means that there are fewer events occurring within each timestep, making it more challenging to extract meaningful patterns from the input data. We believe that these challenges could be addressed in multiple ways. One approach is to explore alternative learning rules that do not require the unrolling of networks in the backward pass, such as forward propagation through time \cite{yin2023accurate} or the forward-forward algorithm \cite{hinton2022forward}. These rules can reduce memory requirements and alleviate the computational burden of training SNNs. Additionally, leveraging the sparsity of SNNs and employing sparse computations during simulation can help reduce memory usage and increase inference speed. Techniques and frameworks that support sparse computations, such as the sparse module in PyTorch\footnote{See torch.sparse at \url{https://pytorch.org/docs/stable/sparse.html} (in beta at the time of writing this document).}, can be utilized for optimization. Lastly, another avenue of exploration is the use of more complex recurrent units, including potentially gated units, to enhance the ability of SNNs to extract meaningful patterns from sparse input data.

\subsection*{Robustifying unsupervised learning}

Unsupervised learning opens the door to backpropagation-free online learning in SNNs. However, an observation from our research is that while unsupervised learning rules can be effective in tasks where prior knowledge can be utilized in the network design, their applicability becomes limited when such knowledge is unavailable or cannot be incorporated into the architecture. For instance, in \chapref{cha:tpami}, we successfully trained an SNN to develop motion selectivity using STDP with a hierarchical architecture that enabled motion information to be discernible as clusters of spatiotemporal patterns. However, unsupervised learning may face challenges solving other tasks. To address this limitation and enhance the controllability of the learning process, we believe that future research on unsupervised learning for SNNs should focus on meta-learning, i.e., learning to learn. Specifically, we propose exploring the learning of the parameters of local learning rules, such as STDP, through either self-supervised or pure supervised approaches. By enabling the network to optimize its online learning rule in an offline training phase according to the needs of the task at hand, there is potential to enhance the robustness and deployability of SNNs. It is worth noting that some neuromorphic processors, such as Intel's Loihi \cite{davies2018loihi, orchard2021efficient}, already offer support for online learning through customizable local learning rules.

\subsection*{The future of neuromorphic flying robotics}

One of the primary contributions of this dissertation is the successful demonstration of a neuromorphic vision-to-control pipeline for controlling a freely flying robot with minimal latency and power consumption. This achievement represents a significant step forward in the field of event-based cameras and SNNs, and it marks the beginning of a long journey towards the development of fully neuromorphic, small (flying) robots. The development of such robots, surpassing the capabilities of their counterparts equipped with conventional sensors and processors, holds great promise for the future. However, to realize this vision, several challenges need to be addressed, both in terms of hardware and software. On the hardware side, a significant advancement could come from improving input/output (I/O) bandwidth in the processors. Enhancing the I/O capabilities can facilitate efficient data transfer between event-based sensors and neuromorphic processors, enabling faster and more seamless processing. Additionally, considerations for small form factors and low power consumption are essential for both the sensors and processors. Furthermore, there is potential for further efficiency gains by transitioning to analog neuromorphic processors, but this will pose even larger challenges in terms of their development and deployment. Regarding software, we reiterate the importance of per-event processing with SNNs, the need for scalable training pipelines and more complex recurrent units, and the potential of meta-learning. Once these challenges in hardware and software are overcome, the journey towards fully neuromorphic robots will gain significant momentum, enabling advancements in various areas of robotics and paving the way for a new era of intelligent and efficient machines.
